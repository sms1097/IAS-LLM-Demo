{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging an LLM for Customer Experience Tracking\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "One Travel is facing a significant challenge quantitatively measuring customer feedback, especially when trying to capture subjective sentiments expressed in customer reviews. The need to answer specific questions, such as the performance of a new boarding process, changes in seating, and other common airline related changes.\n",
    "\n",
    "They don't want to use surveys. Surveys are known to create different types of biases, and when large organizations like airlines have multiple policy changes it can be hard to capture all relevant feedback in one survey, leading to low completion rates. Surveys also don't allow you to go back and track different concepts as they change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Innovative Solution\n",
    "One Travel instead opts to use a Large Language Model (LLM) to track their feedback and the relevant categories. Instead of using a survey and asking a question for the customer to respond to, they instead ask the customer to write about their trip and explain what is top of mind. They then use the LLM to determine if the customer menitoned their category they are tracking and record what the customer experience was. Let's walk through what this approach could look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENAI_KEY = os.environ[\"OPENAI_KEY\"]\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "TOGETHER_API_KEY = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "\n",
    "GPT3 = \"gpt-3.5-turbo-1106\"\n",
    "GPT4 = \"gpt-4-1106-preview\"\n",
    "ZEPHYR = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "@retry(wait=wait_fixed(15), stop=stop_after_attempt(4))\n",
    "def llm(user_prompt, model, temperature=0.3):\n",
    "    model_kwargs = {\"temperature\": temperature}\n",
    "    user_prompt = user_prompt[:3700]\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=False,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "\n",
    "    output = response.choices[0].message.content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = OpenAI(\n",
    "    api_key=TOGETHER_API_KEY,\n",
    "    base_url=\"https://api.together.xyz\",\n",
    ")\n",
    "\n",
    "\n",
    "def llm(user_prompt, model=\"mixtral\", temperature=0.1):\n",
    "    model_kwargs = {\"temperature\": temperature}\n",
    "    user_prompt = user_prompt[:3700]\n",
    "\n",
    "    chat_completion = CLIENT.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            },\n",
    "        ],\n",
    "        model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        # model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\",\n",
    "        # model='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "        **model_kwargs\n",
    "    )\n",
    "    output = chat_completion.choices[0].message.content\n",
    "    return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose Categories\n",
    "The business stakeholders choose the categories they would like to track. This can be any arbitrarily changed and augmented as time goes one, making this solution very flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"seat_comfort\",\n",
    "    \"cabin_staff_service\",\n",
    "    \"food_and_beverages\",\n",
    "    \"inflight_entertainment\",\n",
    "    \"ground_service\",\n",
    "    \"wifi_and_connectivity\",\n",
    "    \"value_for_money\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load in Some Comments\n",
    "Next we load in reviews from some airline passengers. This data can come from wherever your organization stores its data. In this case we just load from a file that we have on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Not Verified | San Francisco to New York. Firs...</td>\n",
       "      <td>seat_comfort</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>✅ Trip Verified |  Puerto Vallarta to Houston....</td>\n",
       "      <td>cabin_staff_service</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>✅ Trip Verified |  Newark to Austin. The worst...</td>\n",
       "      <td>inflight_entertainment</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>✅ Verified Review |  Chicago O'Hare to Denver....</td>\n",
       "      <td>inflight_entertainment</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>My husband booked our flight out of Gulfport M...</td>\n",
       "      <td>seat_comfort</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  \\\n",
       "161   Not Verified | San Francisco to New York. Firs...   \n",
       "621   ✅ Trip Verified |  Puerto Vallarta to Houston....   \n",
       "1975  ✅ Trip Verified |  Newark to Austin. The worst...   \n",
       "1875  ✅ Verified Review |  Chicago O'Hare to Denver....   \n",
       "476   My husband booked our flight out of Gulfport M...   \n",
       "\n",
       "                    category sentiment  \n",
       "161             seat_comfort  Negative  \n",
       "621      cabin_staff_service  Negative  \n",
       "1975  inflight_entertainment   Neutral  \n",
       "1875  inflight_entertainment  Negative  \n",
       "476             seat_comfort  Negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[['review', 'category', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tag the Comments\n",
    "Now that we have the reviews loaded we can ask the LLM to review the information. Here's what that looks like. Notice the advantage that we can handle this problem with plain text. With a small amount of training, any non-technical stakeholder could easily extend this system to handle slightly different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_prompt_template = lambda review: f\"\"\"Here's a customer review for an experience they had on an airline.\n",
    "For each of the following categories decide if the customer's sentiment is Positive, Negative, or Neutral.\n",
    "If a category is not mentioned return \"N/A\".\n",
    "The intended airline is Untied Airlines.\n",
    "\n",
    "Return using ONLY the following output schema.\n",
    "- seat_comfort: <sentiment>\n",
    "- cabin_staff_service: <sentiment>\n",
    "- food_and_beverages: <sentiment>\n",
    "- inflight_entertainment: <sentiment>\n",
    "- ground_service: <sentiment>\n",
    "- wifi_and_connectivity: <sentiment>\n",
    "- value_for_money: <sentiment>\n",
    "\n",
    "Return only in the above format.\n",
    "\n",
    "Review: {review}\n",
    "Output: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(llm_output, categories):\n",
    "    output = {}\n",
    "    category_reviews = [x.lstrip(\"- \") for x in llm_output.split(\"\\n\")]\n",
    "    for category_name, review in zip(categories, category_reviews):\n",
    "        # format the information\n",
    "        rating=review.replace(f\"{category_name}: \", '').split('(')[0]\n",
    "        output[f\"{category_name} pred\"] = rating\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seat_comfort pred': 'N/A',\n",
       " 'cabin_staff_service pred': 'Negative',\n",
       " 'food_and_beverages pred': 'N/A',\n",
       " 'inflight_entertainment pred': 'N/A',\n",
       " 'ground_service pred': 'Negative',\n",
       " 'wifi_and_connectivity pred': 'N/A',\n",
       " 'value_for_money pred': 'N/A'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.review.sample(1).iloc[0]\n",
    "prompt = tag_prompt_template(tmp)\n",
    "output = llm(prompt)\n",
    "parse_output(output, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Report on Changes in Comments\n",
    "\n",
    "This is a really good use case for Prediction-Powered Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "from scipy.stats import binom, norm\n",
    "\n",
    "\n",
    "def binomial_iid(N, alpha, muhat):\n",
    "    def invert_upper_tail(mu):\n",
    "        return binom.cdf(N * muhat, N, mu) - (alpha / 2)\n",
    "\n",
    "    def invert_lower_tail(mu):\n",
    "        return binom.cdf(N * muhat, N, mu) - (1 - alpha / 2)\n",
    "\n",
    "    u = brentq(invert_upper_tail, 0, 1)\n",
    "    l = brentq(invert_lower_tail, 0, 1)\n",
    "    return np.array([l, u])\n",
    "\n",
    "\n",
    "def pp_mean_iid_asymptotic(Y_labeled, Yhat_labeled, Yhat_unlabeled, alpha):\n",
    "    n = Y_labeled.shape[0]\n",
    "    N = Yhat_unlabeled.shape[0]\n",
    "    tildethetaf = Yhat_unlabeled.mean()\n",
    "    rechat = (Yhat_labeled - Y_labeled).mean()\n",
    "    thetahatPP = tildethetaf - rechat\n",
    "    sigmaftilde = np.std(Yhat_unlabeled)\n",
    "    sigmarec = np.std(Yhat_labeled - Y_labeled)\n",
    "    hw = norm.ppf(1 - alpha / 2) * np.sqrt((sigmaftilde**2 / N) + (sigmarec**2 / n))\n",
    "    return [thetahatPP - hw, thetahatPP + hw]\n",
    "\n",
    "\n",
    "def calculate_ppi(Y_labeled, Yhat_labeled, Yhat_unlabeled, alpha, num_trials=100):\n",
    "    n_max = Y_labeled.shape[0]\n",
    "    # ns = np.linspace(100,n_max,20).astype(int)\n",
    "    ns = np.linspace(0, n_max, 20).astype(int)\n",
    "\n",
    "    # Imputed-only estimate\n",
    "    imputed_estimate = (Yhat_labeled.sum() + Yhat_unlabeled.sum()) / (\n",
    "        Yhat_labeled.shape[0] + Yhat_unlabeled.shape[0]\n",
    "    )\n",
    "\n",
    "    # Run prediction-powered inference and classical inference for many values of n\n",
    "    ci = np.zeros((num_trials, ns.shape[0], 2))\n",
    "    ci_classical = np.zeros((num_trials, ns.shape[0], 2))\n",
    "    for i in tqdm(range(ns.shape[0])):\n",
    "        for j in range(num_trials):\n",
    "            # Prediction-Powered Inference\n",
    "            n = ns[i]\n",
    "            rand_idx = np.random.permutation(n)\n",
    "            f = Yhat_labeled.astype(float)[rand_idx[:n]]\n",
    "            y = Y_labeled.astype(float)[rand_idx[:n]]\n",
    "            output = pp_mean_iid_asymptotic(y, f, Yhat_unlabeled, alpha)\n",
    "            ci[j, i, :] = output\n",
    "            # Classical interval\n",
    "            try:\n",
    "                ci_classical[j, i, :] = binomial_iid(n, alpha, y.mean())\n",
    "            except:\n",
    "                avg_ci_classical = None\n",
    "\n",
    "    avg_ci = ci.mean(axis=0)[-1]\n",
    "\n",
    "    try:\n",
    "        ci_imputed = binomial_iid(Yhat_unlabeled.shape[0], alpha, imputed_estimate)\n",
    "    except:\n",
    "        ci_imputed = None\n",
    "    try:\n",
    "        avg_ci_classical = ci_classical.mean(axis=0)[-1]\n",
    "    except:\n",
    "        avg_ci_classical = None\n",
    "\n",
    "    return avg_ci, avg_ci_classical, ci_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/n02_9ld51cl6fd_hblg7_3500000gn/T/ipykernel_66367/2283934634.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_df['sentiment'] = category_df.sentiment.map({'Positive': 1, 'Negative': 0})\n",
      "/var/folders/3h/n02_9ld51cl6fd_hblg7_3500000gn/T/ipykernel_66367/2283934634.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_df[model] = category_df[model].map({'Positive': 1, 'Negative': 0})\n"
     ]
    }
   ],
   "source": [
    "c = categories[1]\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1_pred\"\n",
    "category_df = df[(df.category == c) & (~df[model].isna()) & (~df.sentiment.isna())]\n",
    "category_df['sentiment'] = category_df.sentiment.map({'Positive': 1, 'Negative': 0})\n",
    "category_df[model] = category_df[model].map({'Positive': 1, 'Negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = category_df[~category_df.sentiment.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = category_df.sample(frac=0.1)\n",
    "y_labeled = observed['sentiment'].to_numpy()\n",
    "y_hat_labeled = observed[model].to_numpy()\n",
    "y_hat_unlabeled = category_df[~category_df.index.isin(observed.index)][model].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/var/folders/3h/n02_9ld51cl6fd_hblg7_3500000gn/T/ipykernel_66367/3544698667.py:21: RuntimeWarning: Mean of empty slice.\n",
      "  rechat = (Yhat_labeled - Y_labeled).mean()\n",
      "/Users/seansmith/anaconda3/envs/airline/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/seansmith/anaconda3/envs/airline/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/seansmith/anaconda3/envs/airline/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/seansmith/anaconda3/envs/airline/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/3h/n02_9ld51cl6fd_hblg7_3500000gn/T/ipykernel_66367/3544698667.py:53: RuntimeWarning: Mean of empty slice.\n",
      "  ci_classical[j, i, :] = binomial_iid(n, alpha, y.mean())\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([nan, nan]), array([0.1303768 , 0.39326154]), None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ppi(y_labeled, y_hat_labeled, y_hat_unlabeled, 0.05, num_trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_category_accuracies(model):\n",
    "    for category_name in categories:\n",
    "        tmp_df = df[\n",
    "            (~df[model].isna())\n",
    "            & (~df['sentiment'].isna())\n",
    "            & (df[\"category\"] == category_name)\n",
    "        ]\n",
    "        acc = tmp_df[tmp_df[model] == tmp_df['sentiment']].shape[0] / tmp_df.shape[0]\n",
    "        print(f'{category_name} Accuracy: {round(acc,4)}  (n={tmp_df.shape[0]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat_comfort Accuracy: 0.62  (n=150)\n",
      "cabin_staff_service Accuracy: 0.7273  (n=396)\n",
      "food_and_beverages Accuracy: 0.6818  (n=154)\n",
      "inflight_entertainment Accuracy: 0.6796  (n=103)\n",
      "ground_service Accuracy: 0.7208  (n=394)\n",
      "wifi_and_connectivity Accuracy: 0.4643  (n=56)\n",
      "value_for_money Accuracy: 0.8792  (n=389)\n"
     ]
    }
   ],
   "source": [
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1_pred\"\n",
    "score_category_accuracies(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat_comfort Accuracy: 0.8447  (n=309)\n",
      "cabin_staff_service Accuracy: 0.8223  (n=484)\n",
      "food_and_beverages Accuracy: 0.7533  (n=150)\n",
      "inflight_entertainment Accuracy: 0.7812  (n=96)\n",
      "ground_service Accuracy: 0.7809  (n=429)\n",
      "wifi_and_connectivity Accuracy: 0.7222  (n=36)\n",
      "value_for_money Accuracy: 0.9071  (n=366)\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo-1106_pred\"\n",
    "score_category_accuracies(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat_comfort Accuracy: 0.7736  (n=212)\n",
      "cabin_staff_service Accuracy: 0.8245  (n=416)\n",
      "food_and_beverages Accuracy: 0.7534  (n=146)\n",
      "inflight_entertainment Accuracy: 0.8172  (n=93)\n",
      "ground_service Accuracy: 0.788  (n=401)\n",
      "wifi_and_connectivity Accuracy: 0.7179  (n=39)\n",
      "value_for_money Accuracy: 0.9106  (n=425)\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-4-1106-preview_pred\"\n",
    "score_category_accuracies(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat_comfort Accuracy: 0.798  (n=500)\n",
      "cabin_staff_service Accuracy: 0.718  (n=500)\n",
      "food_and_beverages Accuracy: 0.6124  (n=498)\n",
      "inflight_entertainment Accuracy: 0.501  (n=497)\n",
      "ground_service Accuracy: 0.674  (n=497)\n",
      "wifi_and_connectivity Accuracy: 0.497  (n=497)\n",
      "value_for_money Accuracy: 0.841  (n=497)\n"
     ]
    }
   ],
   "source": [
    "model = 'small_model_predictions'\n",
    "score_category_accuracies(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Flexibly Change the Output Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_prompt_template = lambda review: f\"\"\"Here's a customer review for an experience they had on an airline.\n",
    "For each of the following categories decide if the customer's sentiment is Positive, Negative, or Neutral.\n",
    "If a category is not mentioned return \"N/A\".\n",
    "The intended airline is Untied Airlines.\n",
    "\n",
    "Return using ONLY the following output schema.\n",
    "- seat_comfort: <sentiment>\n",
    "- cabin_staff_service: <sentiment>\n",
    "- food_and_beverages: <sentiment>\n",
    "- inflight_entertainment: <sentiment>\n",
    "- ground_service: <sentiment>\n",
    "- wifi_and_connectivity: <sentiment>\n",
    "- value_for_money: <sentiment>\n",
    "\n",
    "Return only in the above format.\n",
    "\n",
    "Review: {review}\n",
    "Output: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seat_comfort pred': 'N/A',\n",
       " 'cabin_staff_service pred': 'Negative',\n",
       " 'food_and_beverages pred': 'N/A',\n",
       " 'inflight_entertainment pred': 'N/A',\n",
       " 'ground_service pred': 'Negative',\n",
       " 'wifi_and_connectivity pred': 'N/A',\n",
       " 'value_for_money pred': 'Negative'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.review.sample(1).iloc[0]\n",
    "prompt = tag_prompt_template(tmp)\n",
    "output = llm(prompt, GPT3)\n",
    "parse_output(output, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Understand the Problems\n",
    "- We may wish to understand the problems the customer is facing\n",
    "- We can accomplish that by mining for information where the customer is unhappy\n",
    "- There exist more efficient methods to do this, we have a flexible exploratory tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_extraction_prompt_template = lambda review, category, sentiment: f\"\"\"This customer had a {sentiment} experience regarding {category}.\n",
    "In less than 10 words describe the problem related to {category}.\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "\n",
    "Short Review:\"\"\"\n",
    "\n",
    "problem_summarization_prompt_template = lambda problem_str, sentiment, category: f\"\"\"Here are some statements from customers that visited our airline.\n",
    "Please break down the main themes that summarize what was {sentiment} about their experience with {category}.\n",
    "Format this as a report in mardkwon for an expert that needs to make a decision about how to improve customer experience.\n",
    "\n",
    "Problems:\n",
    "{problem_str}\n",
    "\n",
    "Report:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_report(df, category, sentiment, sample_size=40):\n",
    "    reviews = df[(df['category'] == category) & (df['sentiment'] == sentiment)].review.sample(sample_size).tolist()\n",
    "    problem_prompts = [problem_extraction_prompt_template(r, category, sentiment) for r in reviews]\n",
    "\n",
    "    problems = [llm(p, GPT3) for p in tqdm(problem_prompts, desc='Extracting problems')]\n",
    "    problem_str = '\\n'.join(problems)\n",
    "    report_prompt = problem_summarization_prompt_template(problem_str, sentiment, category)\n",
    "    report = llm(report_prompt, GPT3)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Extracting problems: 100%|██████████| 40/40 [01:00<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "report = draft_report(df, 'cabin_staff_service', 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After analyzing customer feedback, it is clear that the main themes regarding negative experiences with cabin staff service are unprofessional behavior, rude and dismissive attitude, poor customer service, lack of assistance and empathy, and inadequate communication. Customers have also mentioned issues with delayed flights, uncomfortable seats, lack of amenities, poor food quality, and cramped seating. There are also concerns about unorganized boarding processes, overbooking, split seats, and mishandling of luggage. Additionally, there are complaints about the handling of flight cancellations, lack of compensation, and unhelpful policies. It is evident that there is a need for improvement in the behavior and professionalism of the cabin staff, as well as in the overall customer service and communication. Measures should also be taken to address issues related to flight delays, seating arrangements, luggage handling, and the overall comfort and amenities provided to passengers.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
